{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "dict_path = '/home/aistudio/data/data130495/dict.txt'\n",
    "data_sat = '/home/aistudio/data/data130495/data1.txt'  # 'work/data2.txt'中文\n",
    "\n",
    "#创建数据字典，存放位置：dicts.txt。在生成之前先清空dict.txt\n",
    "#在生成all_data.txt之前，首先将其清空\n",
    "with open(dict_path, 'w') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate() \n",
    "\n",
    "\n",
    "dict_set = set()\n",
    "train_data = open(data_sat)\n",
    "for data in train_data:\n",
    "    seg = jieba.lcut(data)  #转化为一个列表\n",
    "    for datas in seg:\n",
    "        if not datas is \" \":\n",
    "            if not datas is '\\n':\n",
    "                dict_set.add(datas)\n",
    "\n",
    "dicts = open(dict_path,'w')\n",
    "for data in dict_set:\n",
    "    dicts.write(data + '\\n')\n",
    "dicts.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = {}\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\").split(\"\\t\")[0]\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "vocab = load_vocab(dict_path)\n",
    "\n",
    "for k, v in vocab.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = open(data_sat)\n",
    "for data in train_data:\n",
    "    \n",
    "    input_ids = []\n",
    "    input_names = ''\n",
    "    for token in jieba.cut(data):\n",
    "        # print(token)\n",
    "        # break\n",
    "        if not token is \" \":\n",
    "            if not token is '\\n':\n",
    "                token_id = vocab.get(token, 1)\n",
    "                input_ids.append(token_id)\n",
    "                input_names += token\n",
    "                input_names += ' '\n",
    "    print(input_names)\n",
    "    print(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a31977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot编码\n",
    "import numpy as np\n",
    "#初始数据：每个样本是列表的一个元素（本例中的样本是一个句子，但也可以是一整篇文档）\n",
    "train_data = open('/home/aistudio/data/data130495/data2.txt')\n",
    "samples = []\n",
    "for data in  train_data:\n",
    "    samples.append(data)\n",
    "print(samples)\n",
    "\n",
    "# samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "#构建数据中所有标记的索引\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    #利用 split 方法对样本进行分词。在实际应用中，还需要从样本中去掉标点和特殊字符\n",
    "    for word in jieba.lcut(sample):\n",
    "        # print(word)\n",
    "        if word not in token_index:\n",
    "            #为每个唯一单词指定一个唯一索引。注意，没有为索引编号 0 指定单词\n",
    "            token_index[word] = len(token_index) + 1\n",
    "print(token_index)\n",
    "#对样本进行分词。只考虑每个样本前 max_length 个单词\n",
    "max_length = 10\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    # print(sample)\n",
    "    # datas = jieba.lcut(sample)\n",
    "    #每句话只取10个单词\n",
    "    for j, word in list(enumerate(jieba.lcut(sample)))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        #将结果保存在 results 中\n",
    "        results[i, j, index] = 1.\n",
    "for i in range(len(samples)):\n",
    "    print(f'data:{samples[i][:-1]}')\n",
    "    print(f'ont-hot:{results[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF编码\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "def sklearn_tfidf():\n",
    "    train_data = open('work/data1.txt')\n",
    "    samples = []\n",
    "    for data in  train_data:\n",
    "        samples.append(data)\n",
    "    \n",
    "    vectorizer = CountVectorizer() #将文本中的词语转换为词频矩阵  \n",
    "    X = vectorizer.fit_transform(samples) #计算个词语出现的次数\n",
    "       \n",
    "    transformer = TfidfTransformer()  \n",
    "    tfidf = transformer.fit_transform(X)  #将词频矩阵X统计成TF-IDF值 \n",
    "    for i in range(len(samples)):\n",
    "        print(samples[i])\n",
    "        print(tfidf.toarray()[i])\n",
    "sklearn_tfidf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
